{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src= \"./resources/title.png\">\n",
    "<img src= \"./resources/muchmath.png\" style = \"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Objective:\n",
    "Students will be able to **define** linear algebra's role in data science and **describe** a few _key concepts_ and _rules_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start with...\n",
    "<img src= \"./resources/algebra.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algebra's from highschool, but that doesn't make it simple\n",
    "\n",
    "#### Problem 1:\n",
    "Solve for $x$</br>\n",
    "\n",
    "$20  = 5 + 3x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2:\n",
    "Solve for $x$</br>\n",
    "\n",
    "$20 - 7x = 6x - 6$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 3\n",
    "Solve for $x$ and $y$</br>\n",
    "\n",
    "$-2(x - 1) + 4y = 5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4\n",
    "Solve for $x_1$ and $x_2$</br>\n",
    "\n",
    "$4x_1 + 2x_2 = 8$</br>\n",
    "\n",
    "$5x_1 + 3x_2 = 9$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are all these problems doing?\n",
    "\n",
    "solving for the _unknown_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order of magnitude\n",
    "\n",
    "Now Problem 4 might be doable by hand, but what if instead of 2 equations we had 5? 20? 200? 50,000?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fortunately for us we have\n",
    "\n",
    "<img src= \"https://media0.giphy.com/media/JlxFcvNuzlPYA/giphy.gif?cid=790b7611c4a4fc74c05cd06fe2c8cc00860e04b6f8049e52&rid=giphy.gif\">\n",
    "\n",
    "## Computers!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But there is a problem:\n",
    "\n",
    "| people | computers|\n",
    "|--------|----------|\n",
    "|can read equations like sentences | can't really do that |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear albegra solves that problem, by turning this:\n",
    "\n",
    "$4x_1 + 2x_2 = 8$</br>\n",
    "\n",
    "$5x_1 + 3x_2 = 9$\n",
    "\n",
    "### into this:\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}4 & 2 \\\\ 5 & 3 \\end{bmatrix}*\\begin{bmatrix}x_1\\\\x_2\\end{bmatrix} = \\begin{bmatrix}8\\\\9\\end{bmatrix}\n",
    "$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise, how would we rewrite the equation sets in each problem into linear algebra?\n",
    "\n",
    "#### Problem 1\n",
    "\n",
    "$x_0 + 2x_1 = 10$</br>\n",
    "\n",
    "$3x_0 + x_1 = 9$\n",
    "\n",
    "#### Problem 2\n",
    "\n",
    "$x_0 + 2x_1 = 10$</br>\n",
    "\n",
    "$3x_0 + x_1 = 9$</br>\n",
    "\n",
    "$32x_0 - 6x_1 = 24$\n",
    "\n",
    "#### Problem 3\n",
    "$x_0 + 2x_1 = 10$</br>\n",
    "\n",
    "$3x_0 + x_1 + 5x_2= 22$</br>\n",
    "\n",
    "$32x_0 - 6x_1 -4x_2= 7$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We should probably learn some vocabulary for what we are using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scalar\n",
    "\n",
    "$ 2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector $\\vec{v}$\n",
    "\n",
    "$\\begin{bmatrix}8\\\\9\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what if I told you that **both** $a$ and $b$ are vectors?\n",
    "\n",
    "$a = \\begin{bmatrix}8\\\\9\\end{bmatrix} \\\\              \n",
    "b = \\begin{bmatrix}8 & 9\\end{bmatrix}$\n",
    "\n",
    "How are they alike?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix\n",
    "$ \\begin{bmatrix}4 & 2 \\\\ 5 & 3 \\end{bmatrix} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor\n",
    "\n",
    "$ \\left[ \\begin{array}{ccc} \n",
    "         \\begin{bmatrix}4 & 2 \\\\ 5 & 3 \\end{bmatrix} &\n",
    "         \\begin{bmatrix}6 & -4 \\\\ 2 & 8 \\end{bmatrix} \\\\ \n",
    "         \\begin{bmatrix}-1 & 5 \\\\ 0 & 1 \\end{bmatrix} & \n",
    "         \\begin{bmatrix}9 & -2 \\\\ -5 & 4/5 \\end{bmatrix}  \\end{array} \\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Or put differently:\n",
    "\n",
    "<img src = \"./resources/datadogs.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific definitions of Data Types for Linear Algebra\n",
    "\n",
    "* **Scalars** only have magnitude.\n",
    "\n",
    "* A **vector** is an array with **magnitude and direction**.\n",
    "  - The coordinates of a vector represent where the tip of the vector would be if you travelled from the origin\n",
    "  - The magnitude of a vector would be its length in space.\n",
    "\n",
    "* **Matrices** can be interpreted differently in different contexts but it's often used to represent multiple simultaneous vectors. \n",
    "\n",
    "* **Tensors** are made up of matrices with the same dimensions.\n",
    "\n",
    "* A vector or matrix can be multiplied by a scalar to create a change in **scale** and/or **direction**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick code break!\n",
    "For linear algebra, `NumPy` is your favorite package.\n",
    "\n",
    "Vectors, matrices and tensors are represented by NumPy arrays. **Not lists!!!** <br>\n",
    "\n",
    "We can use `np.array.shape` to explore the dimensions of these data structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make some objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = np.array([1, 2, 3, 4, 5, 6])\n",
    "matrix1 = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "matrix2 = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "tensor = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print them out and find their shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vector)\n",
    "print('vector shape:', vector.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matrix1)\n",
    "print('matrix1 shape:', matrix1.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matrix2)\n",
    "print('matrix2 shape:', matrix2.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tensor)\n",
    "print('tensor shape:', tensor.shape, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: How would you index or subset a vector, matrix, or tensor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise:\n",
    "\n",
    "Index each object to return the **6** for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Okay, let's get back to the....\n",
    "\n",
    "<img src= \"./resources/linear.png\">\n",
    "\n",
    "## part of Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Linear Equations?\n",
    "\n",
    "Linear equations only have **linear variables**. This means our unknowns are only multiplied by a scalar and raised to a power of only **one**, such as:\n",
    "\n",
    "$ x - 2y = 1$\n",
    "\n",
    "$3ex + 2\\pi y = 0$\n",
    "\n",
    "**Not linear:**\n",
    "\n",
    "$ x^2 - 2\\ln{y} = 4$\n",
    "\n",
    "$0.5x + 2y^x = 11$\n",
    "\n",
    "$e^x + 2x=2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression built upon Linear Algebra\n",
    "A linear regression can be interpreted as the solution to a system of linear equations: each observation just corresponds to a linear equation, and the **coefficients** are the linear unknowns we're solving for! \n",
    "\n",
    "We're representing each **observation** as a **linear combination of features**.\n",
    "\n",
    "Our prediction equation for a linear regression typically looks something like:\n",
    "\n",
    "$ y_{pred} = \\beta_{0} + \\beta_{1}x_1 + \\beta_{2}x_2 + ... + \\beta_{n}x_n $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In matrix notation that can also be:\n",
    "\n",
    "$ y = Xb $, so we are solving for $b$.\n",
    "\n",
    "Where:\n",
    "- $X$ is your matrix of scalars\n",
    "- $b$ is the vector of coefficients\n",
    "\n",
    "Okay, specifically we are solving for $\\hat{b}$:\n",
    "\n",
    "$ \\hat{y} = X\\hat{b}$\n",
    "\n",
    "to:\n",
    "\n",
    "$\\min\\Sigma{(\\hat{y} - y)^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pause\n",
    "\n",
    "<img src= \"https://i0.wp.com/timemanagementninja.com/wp-content/uploads/2014/02/Pause-Button-Key.jpg?w=600&ssl=1\">\n",
    "\n",
    "## That was a lot, let's make sure everyone followed with that knowledge drop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra powers the majority of machine learning algorithms we will learn in this course\n",
    "<img src= \"./resources/linearalgebra.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next lecture we will review an example of regression using linear algebra, but this lecture is about terminology.</br>\n",
    "You will see a few recurring types of matricies and vectors accross algorithms, so next:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MVPs of Linear Algebra\n",
    "<img src = \"./resources/mvp.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Identity Matrix\n",
    "An identity matrix is a square with a diagonal of 1's moving from left to right and the remaining numbers 0. When a matrix is multiplied by an identity matrix, it will result in the same matrix (think of it as the operational equivalent to 1 for linear algebra).\n",
    "\n",
    "<img src = \"./resources/identity_matrix.svg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_3 = np.identity(3)\n",
    "print(i_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Matrix Inverse\n",
    "The **inverse** of a matrix, when a matrix is multiplied by its inverse, it results in the identity matrix. \n",
    "\n",
    "<img src = \"./resources/inverse.webp\">\n",
    "\n",
    "The order of multiplication does not matter for a matrix and its inverse:\n",
    "\n",
    "$$A \\cdot A^{-1} = A^{-1} \\cdot A $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original matrix\n",
    "x = np.array([[4,8,10],[3,9,12],[5,10,15]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse of x and multiplying by x\n",
    "inv_x = np.linalg.inv(x)\n",
    "print(inv_x, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if it produces the identity matrix:\n",
    "print(np.round(x.dot(inv_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a Do all matricies have an inverse?  Nope. \n",
    "\n",
    "    An n-by-n square matrix A is called invertible if there exists an N by N square matrix B such that\n",
    "\n",
    "<div style=\"text-align:center\"><span style=\"color:blue; font-family:Georgia; font-size:1.5em;\">AB = BA = I</span></div>\n",
    "\n",
    "    where I is the identity matrix. A and B are inverses of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait, what was that last function `x.dot` ?\n",
    "### 3. Dot product\n",
    "\n",
    "The dot product of matrices is also commonly known as **Matrix Multiplication**. Unless otherwise stated, _multiplication_ refers to this kind of multiplication.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "a_{1,1} & a_{1,2} \\\\\n",
    "a_{2,1} & a_{2,2}\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    "b_{1,1} & b_{1,2} \\\\\n",
    "b_{2,1} & b_{2,2}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{1,1}\\times b_{1,1} + a_{1,2}\\times b_{2,1} & a_{1,1}\\times b_{1,2} + a_{1,2}\\times b_{2,2} \\\\\n",
    "a_{2,1}\\times b_{1,1} + a_{2,2}\\times b_{2,1} & a_{2,1}\\times b_{1,2} + a_{2,2}\\times b_{2,2}\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "<img src= \"./resources/matrix_mult.png\" style=\"width: 400px;\">\n",
    "https://www.mathsisfun.com/algebra/matrix-multiplying.html\n",
    "\n",
    "#### Dot product rules:\n",
    "- We take the **rows** (horizontal) of the first matrix and do an element-wise product with the **columns** (vertical) of the second matrix.\n",
    "- Order of operations matters, $AB ≠ BA $  and $(AB)C ≠ A(BC)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise:\n",
    "\n",
    "Let's do one small dot product by hand! (this is the most matrix math you will be asked to do)\n",
    "\n",
    "$\\begin{bmatrix}8\\\\5\\\\6\\end{bmatrix} * \\begin{bmatrix}3 & 4 & 2 \\end{bmatrix}  = ?$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Transpose\n",
    "\n",
    "The _transpose_ of Matrix $X$, or using notation, $X^{T}$, is matrix $X$ in reverse shape order.\n",
    "\n",
    "$a = \\begin{bmatrix}8\\\\9\\end{bmatrix} \\\\              \n",
    "a^T = \\begin{bmatrix}8 & 9\\end{bmatrix}$\n",
    "\n",
    "Calling `.transpose()` on an array **reverses** the shape order of a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the original shape of matrix1\n",
    "print(matrix1)\n",
    "print('matrix1 shape:', matrix1.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transposed\n",
    "print(matrix1.transpose(), '\\n')\n",
    "print('matrix1.transpose() shape:', matrix1.transpose().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also the shorthand function of `.T`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matrix1.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "(again, by hand!)\n",
    "\n",
    "What would be the transpose of the following matrix?\n",
    "\n",
    "$\\begin{bmatrix}8 & 2\\\\5 & 3\\\\6&4\\end{bmatrix} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we care about these MVPs?\n",
    "![gif](https://media1.giphy.com/media/QA7C1yuI0QZtBbxxM4/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get into shape\n",
    "Matrix math cares about the _shape_ of the matricies involved.\n",
    "\n",
    "What have we seen so far with matrix multiplication? what shape does each matrix need to be for matrix multiplication to work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Addition and subtraction - same shape\n",
    "\n",
    "$ \\vec{v} = \\begin{bmatrix}v_{1} \\\\v_{2}\\end{bmatrix} \\vec{w} = \\begin{bmatrix}w_{1} \\\\w_{2}\\end{bmatrix} $\n",
    "\n",
    "$ \\vec{v} + \\vec{w} = \\begin{bmatrix}v_{1} + w_{1} \\\\v_{2} + w_{2}\\end{bmatrix} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do the **complex** math behind the screen and get matricies to the correct shape for the right formula, matricies are transformed using transposes, identity matricies, and plenty of dot products. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait, why do we care about linear algebra again?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear algebra is used everywhere in machine learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression\n",
    "\n",
    "We've already covered it. Anywhere you are using a multi-dimensional dataset and optimizing a cost fuction, or transforming the data - linear algebra is how the calculations are run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Text Analytics\n",
    "It is used to model complicated things like language. </br>\n",
    "Some of you may have heard of \"vectorizing text\" when talking about NLP.\n",
    "\n",
    "Converting words and text into vectors and matricies allows us to see how \"close\" and \"far apart\" words are from eachother in meaning and connection.\n",
    "\n",
    "<img src = \"./resources/Word-Vectors.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Image compression and recognition\n",
    "\n",
    "At its basest form, an image is a three dimensional matrix.\n",
    "\n",
    "An $n$ by $m$ by $3$ matrix to be precise.\n",
    "\n",
    "Where $n$ and $m$ are the size of the image and each pixel is an array of three digits represeting its color code.\n",
    "\n",
    "<img src = \"./resources/images.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Recommendation engines \n",
    "Can make much more sophisticated recommendations by using linear algebra in conjunction with user and content data.\n",
    "\n",
    "**Quick thought exercise** - what would the matrix of user and content data look like?\n",
    "\n",
    "<img src = \"./resources/netflix.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exit Ticket\n",
    "\n",
    "You will see linear algebra again very soon.\n",
    "\n",
    "To close this lecture, let's end with a knowledge check in the form of an exit ticket\n",
    "\n",
    "[QUICK QUIZ HERE!!](https://forms.gle/K196vRawJ2FpTW8h9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Resources\n",
    "* 3 Blue 1 Brown:  https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_a\n",
    "* Matrix approach to Linear Regression: http://www.stat.columbia.edu/~fwood/Teaching/w4315/Fall2009/lecture_11\n",
    "* [link to fun desmos interaction](https://www.desmos.com/calculator/yovo2ro9me)\n",
    "* [Link to good video on scalars and vectors](https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)\n",
    "* [What is X^T * X?](https://stats.stackexchange.com/questions/267948/intuitive-explanation-of-the-xtx-1-term-in-the-variance-of-least-square/267963)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.desmos.com/calculator/y08wwbjwid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only if there is time:\n",
    "\n",
    "(or for those advanced folks who have sped through the rest of the content already)\n",
    "\n",
    "## Linear Regression with Linear Algebra (OLS!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll work through a linear regression problem with the Auto dataset. We want to predict the **mpg** using *cylinders, displacement, horsepower, weight, acceleration and year*.\n",
    "\n",
    "We're representing each **observation** as a **linear combination of features**.\n",
    "\n",
    "Our prediction equation for a linear regression typically looks something like:\n",
    "\n",
    "$ y_{pred} = \\beta_{0} + \\beta_{1}x_1 + \\beta_{2}x_2 + ... + \\beta_{n}x_n $\n",
    "\n",
    "Represented in matrix form:\n",
    "\n",
    "$ y = Xb $, so we are solving for $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "car_df = pd.read_csv('http://faculty.marshall.usc.edu/gareth-james/ISL/Auto.csv',na_values='?').dropna()\n",
    "car_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = car_df[['cylinders','displacement','horsepower','weight','acceleration','year']]\n",
    "y = car_df['mpg']\n",
    "X_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the intercept term\n",
    "X_df['constant'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ y = Xb + 0 $  --> $ y = Xb $\n",
    "\n",
    "We want to solve for $b$! As we did before, to solve for $b$ we need to multiply both sides by the inverse of $X$.\n",
    "\n",
    "Let's try to $ X^{-1} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.inv(X_df.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get: \n",
    "\n",
    "    LinAlgError: Last 2 dimensions of the array must be square.\n",
    "\n",
    "We can only calculate an inverse of a **square** matrix.\n",
    "\n",
    "we can only find the inverse of square matrices. So with $b$ not being square, how can we solve this system using the data that we have? (No spoilers.)\n",
    "\n",
    "\n",
    " $$b = (X^{T}X)^{-1}X^{T}y$$ \n",
    "\n",
    "\n",
    "\n",
    "Let's apply this to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_df.values\n",
    "xt = x.T\n",
    "\n",
    "# We create an squared matrix that we can invert\n",
    "xtx = xt @ x\n",
    "xtx_inv = np.linalg.inv(xtx)\n",
    "\n",
    "product = xtx_inv @ xt\n",
    "\n",
    "b = product @ y.values\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our coefficients! They correspond to each of the columns in `X_df` in order. Let's compare this to our `sklearn` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(X_df.columns, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing with sklearn\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "\n",
    "skl_X = X_df.drop(columns = 'constant')\n",
    "lr.fit(skl_X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('constant: ', lr.intercept_)\n",
    "print('coefficients: ', lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now try yourself adding the remaining variable\n",
    "No copy pasting<br>\n",
    "Write your linear algebra to gain experience<br>\n",
    "How do the coefficients change?<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
